{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/path/to/data/dir'\n",
    "file_pcs = path.join(dir_data, 'patient_code_sequences.txt')\n",
    "file_persons = path.join(dir_data, 'persons.csv')\n",
    "file_concepts = path.join(dir_data, 'concepts.csv')\n",
    "file_sequences = path.join(dir_data, 'patient_sequences.pkl')\n",
    "file_backup_suffix = '.backup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column indices for persons.csv\n",
    "df_persons = pd.read_csv(file_persons, sep='\\t', header=0, index_col=0, \n",
    "                         parse_dates=['birth_date'], infer_datetime_format=True)\n",
    "\n",
    "# Check the data types of the columns\n",
    "df_persons.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concept definitions\n",
    "df_concepts = pd.read_csv(file_concepts, sep='\\t', header=0, index_col='concept_id')\n",
    "df_concepts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep patient sequences into TaggedDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for reading in the patient_code_sequences.txt\n",
    "\n",
    "# Date of occurrence and list of concepts occurring on this date\n",
    "DateOccurrence = namedtuple('DateOccurrence', ['date', 'concept_ids'])\n",
    "\n",
    "def _process_pcs_line(line):\n",
    "    \"\"\" Processes a line from patient_code_sequences.txt and parses out the patient ID\n",
    "    and DateOccurrences \"\"\"\n",
    "    split = line.strip().split('\\t')\n",
    "        \n",
    "    # person_id is the first entry\n",
    "    pid = int(split.pop(0))\n",
    "    \n",
    "    # Process the remaining string into a list of Occurrences\n",
    "    date_occurrences = [_process_date_occurrence_str(x) for x in split]\n",
    "    \n",
    "    return pid, date_occurrences\n",
    "\n",
    "def _process_date_occurrence_str(dos):\n",
    "    \"\"\" Processes a DateOccurrence string \n",
    "    format: YYYY-MM-DD:<list of concept IDs separated by commas> \"\"\"\n",
    "    date_str, concept_ids_str = dos.split(':')\n",
    "    occ = DateOccurrence(datetime.strptime(date_str.strip(), '%Y-%m-%d'), \n",
    "                         [int(x) for x in concept_ids_str.split(',')])\n",
    "    return occ\n",
    "\n",
    "def create_patient_sequences(f_pcs_in, f_seq_out=None, min_seq_length=10, randomize_order=True, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and parses it into sequences for each patient\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pseqs - list of TaggedDocument(words=[concept_ids], tags=[person_id])\n",
    "    pseqs = list()\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the heaer line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "\n",
    "            # Combine sequence of concepts from each date into on sequence for the patient\n",
    "            current_seq = []\n",
    "            for date_occurrence in date_occurrences:\n",
    "                concepts = date_occurrence.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    shuffle(concepts)\n",
    "                    \n",
    "                current_seq += concepts\n",
    "                \n",
    "            if len(current_seq) >= min_seq_length:\n",
    "                pseqs.append(TaggedDocument(words=[str(x) for x in current_seq], tags=[pid]))\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time and size of data structure\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(pseqs, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save the concept age distributions            \n",
    "        pickle.dump(pseqs, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return pseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseqs = create_patient_sequences(file_pcs, f_seq_out=None, min_seq_length=5, randomize_order=True, \n",
    "                                        verbose=True, save_intermediates=False)\n",
    "n_pseqs = len(pseqs)\n",
    "print(n_pseqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_filename(model, epochs=None):\n",
    "    \"\"\" Generate a filename for to save the model using the string representation of the model, \n",
    "    which already includes most of the important model parameters. \"\"\"\n",
    "    f_model = re.sub('[^\\w\\-_\\. ]', '_', str(model))\n",
    "    if epochs:\n",
    "        f_model += f'e{epochs}'\n",
    "    f_model += '.d2v'\n",
    "    return f_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph Vector - Distributed Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec(dm=1, vector_size=100, window=7, min_count=5, alpha=0.023, hs = 0, negative=15, \n",
    "                   epochs=20, workers=6, report_delay=60)\n",
    "\n",
    "# Build Vocab\n",
    "t1 = time()\n",
    "model_dm.build_vocab(pseqs, progress_per=1000000)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Build Vocab Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Train\n",
    "t1 = time()\n",
    "model_dm.train(pseqs, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=60)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Train Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Save the model\n",
    "f_model = path.join(dir_data, model_filename(model_dm, epochs=model_dm.epochs))\n",
    "print(f'Saving model to: {f_model}')\n",
    "model_dm.save(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph Vector - Distributed Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=100, window=7, min_count=5, alpha=0.023, hs = 0, negative=15, \n",
    "                     epochs=20, workers=6, report_delay=60)\n",
    "\n",
    "# Build Vocab\n",
    "t1 = time()\n",
    "model_dbow.build_vocab(pseqs, progress_per=100000)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Build Vocab Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Train\n",
    "t1 = time()\n",
    "model_dbow.train(pseqs, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=60)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Build Vocab Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Save the model\n",
    "f_model = path.join(dir_data, model_filename(model_dbow, epochs=model_dbow.epochs))\n",
    "print(f'Saving model to: {f_model}')\n",
    "model_dbow.save(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-load the models\n",
    "Reload the models if the kernel was shut down after training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec.load(path.join(dir_data, 'Doc2Vec_dm_m_d100_n15_w7_mc5_s0.001_t6_e20.d2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(path.join(dir_data, 'Doc2Vec_dbow_d100_n15_mc5_s0.001_t6_e20.d2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cohort for disease subtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for storing info about a cohort patient sequence\n",
    "# Sequence stored as TaggedDocument object for D2V processing\n",
    "# Will use OMOP concept ID for label\n",
    "CohortPatientSeq = namedtuple('CohortPatientSeq', ['person_id', 'sequence', 'label', 'date_lower', 'date_upper'])\n",
    "\n",
    "def create_cohort_patient_sequences(f_pcs_in, cohort_concepts, f_seq_out=None, time_window=180, randomize_order=True, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and extracts sequences +/- <time_window> days around the \n",
    "    first occurrence of any encountered desired concept\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pcss - cohort patient sequences: list of CohortPatientSeq objects\n",
    "    cpss = list()\n",
    "    count = 0\n",
    "    \n",
    "    # Time window for finding occurrences \n",
    "    time_window = timedelta(days=time_window)\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the heaer line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "            \n",
    "            # Keep track of which concepts we can still try to find for this patient\n",
    "            search_concepts = set(cohort_concepts)\n",
    "\n",
    "            for date_occurrence in date_occurrences:\n",
    "                found_concepts = search_concepts.intersection(set(date_occurrence.concept_ids))\n",
    "                if not found_concepts:           \n",
    "                    # No matching concepts found. Moving on\n",
    "                    continue\n",
    "                    \n",
    "                # Found at least one desired concept. Create a sequence of occurrences within +/- time_window                                \n",
    "                date_lower = date_occurrence.date - time_window\n",
    "                date_upper = date_occurrence.date + time_window\n",
    "                current_seq = list()\n",
    "                for do in date_occurrences:\n",
    "                    if do.date < date_lower:\n",
    "                        continue\n",
    "                        \n",
    "                    if do.date > date_upper:\n",
    "                        # No more date_occurrences within the desired time window.                         \n",
    "                        break\n",
    "                        \n",
    "                    # The date_occurrence is within the time_window. Add occurrences to seq\n",
    "                    concepts = do.concept_ids\n",
    "                    if randomize_order:\n",
    "                        # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                        shuffle(concepts)\n",
    "                    current_seq += concepts\n",
    "                    \n",
    "                # Convert the sequence of OMOP concept IDs to TaggedDocument for D2V processing\n",
    "                tagged_doc_seq = TaggedDocument(words=[str(x) for x in current_seq], tags=[pid])                \n",
    "                    \n",
    "                # Save the sequence along with patient ID, time window, and label\n",
    "                # If more than one matching concept found in this date_occurrence, write them both\n",
    "                for found_concept in found_concepts:\n",
    "                    cps = CohortPatientSeq(pid, tagged_doc_seq, found_concept, date_lower, date_upper)\n",
    "                    cpss.append(cps)\n",
    "                \n",
    "                # Remove the found concepts from the list to search\n",
    "                search_concepts = search_concepts - found_concepts                                              \n",
    "                if len(search_concepts) == 0:\n",
    "                    # No more concepts to search for for this patient\n",
    "                    break\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(cpss, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save the concept age distributions            \n",
    "        pickle.dump(cpss, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return cpss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chronic kidney disease as an example disease domain\n",
    "Automatic selection of known disease subtypes of chronic kidney disease (OMOP:46271022) by selecting all observed concept descendants (using OMOP concept_ancestor table) of CKD with min_levels_of_separation > 0 and max_levels_of_separation <= 1  \n",
    "  \n",
    "OMOP ID: Condition concept name  \n",
    "443614: Chronic kidney disease stage 1\t\n",
    "443601: Chronic kidney disease stage 2\t\n",
    "443597: Chronic kidney disease stage 3\t\n",
    "443612: Chronic kidney disease stage 4\t\n",
    "443611: Chronic kidney disease stage 5\t\n",
    "43531578: Chronic kidney disease due to type 2 diabetes mellitus\t\n",
    "44782429: Chronic kidney disease due to hypertension  \n",
    "  \n",
    "Note: 44782429 was excluded from data extraction since it's considered an iatrogenic code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concepts_ckd = [443614, 443601, 443597, 443612, 443611, 43531578]\n",
    "f_seq_out = path.join(dir_data, 'ckd_cohort_patient_sequences.pkl')\n",
    "cpss = create_cohort_patient_sequences(file_pcs, cohort_concepts=concepts_ckd, f_seq_out=f_seq_out, time_window=180, \n",
    "                                       randomize_order=True, verbose=True, save_intermediates=False)\n",
    "n_cpss = len(cpss)\n",
    "print(n_cpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many examples of each concept we found\n",
    "cohort_counter = Counter()\n",
    "for cps in cpss:\n",
    "    cohort_counter[cps.label] += 1\n",
    "print(cohort_counter)\n",
    "print(len(cohort_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct patients\n",
    "n_patients = len(set([cps.person_id for cps in cpss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load this data if kernel restarted\n",
    "with open(path.join(dir_data, 'ckd_cohort_patient_sequences.pkl'), 'rb') as f:\n",
    "    cpss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create patient vectors for cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to run inference on a patient sequence. Mean of inferred vectors will be used\n",
    "n_inferences = 1\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "# Store the patient vectors in a list. The list should have the same order as cpss\n",
    "patient_vectors = list()\n",
    "for i, cps in enumerate(cpss):\n",
    "    l_dm = list()\n",
    "    l_dbow = list()\n",
    "    \n",
    "    # Take the mean of n_inferences iterations of inferring the vector\n",
    "    for _ in range(n_inferences):\n",
    "        l_dm.append(model_dm.infer_vector(cps.sequence.words))\n",
    "        l_dbow.append(model_dm.infer_vector(cps.sequence.words))\n",
    "    a_dm = np.mean(np.array(l_dm), axis=0)\n",
    "    a_dbow = np.mean(np.array(l_dm), axis=0)\n",
    "    patient_vectors.append(np.concatenate((a_dm, a_dbow)))\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        ellapsed_time = (time() - t1) / 60\n",
    "        print(f'{i}: {ellapsed_time:.02f} min')\n",
    "        \n",
    "# Convert to numpy array\n",
    "patient_vectors = np.array(patient_vectors)\n",
    "        \n",
    "with open(path.join(dir_data, 'ckd_cohort_patient_vectors.pkl'), 'wb') as f:\n",
    "    pickle.dump(patient_vectors, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load this data if kernel restarted\n",
    "with open(path.join(dir_data, 'ckd_cohort_patient_vectors.pkl'), 'rb') as f:\n",
    "    patient_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE using the defined disease subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "\n",
    "tsne = TSNE(n_components=2, metric='cosine', perplexity=50, learning_rate=400, init='pca', \n",
    "            n_iter=2000, n_jobs=8, verbose=2, random_state=42)\n",
    "patient_vectors_tsne = tsne.fit_transform(patient_vectors)\n",
    "\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'{ellapsed_time} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sns bright palette but re-order it so that it goes from purple to red for Stage 1 - 5\n",
    "palette = sns.color_palette('bright', 6)\n",
    "palette_ckd = [palette[i] for i in [2, 4, 1, 5, 0, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Tried DBSCAN and K-means. Could not find good settings for eps and min_samples with DBSCAN that produced good clustering. K-means worked relatively well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different settings\n",
    "for min_samples in [5, 10, 20]:\n",
    "    for eps in [0.35]:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine', n_jobs=8)\n",
    "        clustering = dbscan.fit(patient_vectors)\n",
    "\n",
    "        counter = Counter()\n",
    "        for l in clustering.labels_:\n",
    "            counter[l] += 1\n",
    "\n",
    "        print(f'eps: {eps}, min_samples: {min_samples}')\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.35, min_samples=5, metric='cosine', n_jobs=8)\n",
    "clustering = dbscan.fit(patient_vectors)\n",
    "\n",
    "palette_kmeans = sns.color_palette('bright', len(set(clustering.labels_)))\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.scatterplot(patient_vectors_tsne[:,0], patient_vectors_tsne[:,1], \n",
    "                hue=clustering.labels_, legend='full', palette=palette_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, verbose=1, random_state=42, n_jobs=8)\n",
    "clustering = kmeans.fit(patient_vectors)\n",
    "predicted_labels = clustering.predict(patient_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_kmeans = sns.color_palette('bright', 7)\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.scatterplot(patient_vectors_tsne[:,0], patient_vectors_tsne[:,1], \n",
    "                hue=predicted_labels, legend='full', palette=palette_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known Disease Subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of patient sequences for each known CKD subtype\n",
    "patients_known_disease_subtype = defaultdict(list)\n",
    "for cps in cpss:\n",
    "    patients_known_disease_subtype[cps.label].append(cps)\n",
    "\n",
    "# dict[disease subtype concept_id] => dict[domain_id] => dict[concept_id] => Counter\n",
    "known_disease_profiles = dict()\n",
    "\n",
    "# For each disease subtype, count the number of patients each concept is observed in\n",
    "for known_disease_subtype, cohort_cpss in patients_known_disease_subtype.items():\n",
    "    # dict[domain_id] => Counter\n",
    "    cohort_domain_concept_counter = defaultdict(Counter)\n",
    "    \n",
    "    for cohort_cps in cohort_cpss:\n",
    "        # cohort_cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "        # Convert it to a list of concept_ids (ints)\n",
    "        seq = [int(x) for x in cohort_cps.sequence.words]\n",
    "        \n",
    "        # Keep track of which conpepts we've already seen for this patient so we don't add again\n",
    "        concepts_observed = list()  \n",
    "        \n",
    "        for concept in seq:\n",
    "            if concept in concepts_observed:\n",
    "                # Already seen this concept for this patient\n",
    "                continue            \n",
    "                \n",
    "            domain = df_concepts.loc[concept, 'domain_id']\n",
    "            cohort_domain_concept_counter[domain][concept] += 1\n",
    "            concepts_observed.append(concept)\n",
    "            \n",
    "    known_disease_profiles[known_disease_subtype] = cohort_domain_concept_counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the stats of each concept that is observed in > 20% of each disease subtype\n",
    "print('Note: subtype percentages can add up to > 100% since different timelines can be extracted from each patient to represent a disease subtype')\n",
    "for disease_subtype, cohort_domain_concept_counter in known_disease_profiles.items():        \n",
    "    concept_name = df_concepts.loc[disease_subtype, 'concept_name']\n",
    "    n_disease_subtype = len(patients_known_disease_subtype[disease_subtype])\n",
    "    print(f'{concept_name}: {n_disease_subtype} / {n_patients} = {(n_disease_subtype / n_patients * 100):.02f}')\n",
    "    \n",
    "    for domain, concept_counter in cohort_domain_concept_counter.items():\n",
    "        print(domain)\n",
    "        \n",
    "        # Create a DataFrame from the counts\n",
    "        df_subtype_domain_counts = pd.DataFrame.from_dict(concept_counter, orient='index', columns=['count'])\n",
    "        df_subtype_domain_counts.index.name = 'concept_id'\n",
    "        \n",
    "        # Calculate prevalence of each concept within the disease subtype\n",
    "        df_subtype_domain_counts['prevalence'] = df_subtype_domain_counts['count'] / n_disease_subtype * 100\n",
    "        \n",
    "        # Add concept_name to DataFrame\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.join(df_concepts['concept_name'], how='left')\n",
    "        \n",
    "        # Re-arrange column order,  sort by descending order of count, and display\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts[['concept_name', 'count', 'prevalence']]\n",
    "        df_subtype_domain_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "        display(df_subtype_domain_counts.loc[((df_subtype_domain_counts['prevalence'] > 20) & (df_subtype_domain_counts['count'] > 20)), :].head(20))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Disease Subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of patient sequences for each predicted (learned) disease subtype\n",
    "patients_learned_disease_subtype = defaultdict(list)\n",
    "for i, cps in enumerate(cpss):\n",
    "    patients_learned_disease_subtype[predicted_labels[i]].append(cps)\n",
    "\n",
    "# dict[disease subtype concept_id] => dict[domain_id] => dict[concept_id] => Counter\n",
    "learned_disease_profiles = dict()\n",
    "\n",
    "# For each disease subtype, count the number of patients each concept is observed in\n",
    "for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "    # dict[domain_id] => Counter\n",
    "    cohort_domain_concept_counter = defaultdict(Counter)\n",
    "    \n",
    "    for cohort_cps in cohort_cpss:\n",
    "        # cohort_cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "        # Convert it to a list of concept_ids (ints)\n",
    "        seq = [int(x) for x in cohort_cps.sequence.words]\n",
    "        \n",
    "        # Keep track of which conpepts we've already seen for this patient so we don't add again\n",
    "        concepts_observed = list()  \n",
    "        \n",
    "        for concept in seq:\n",
    "            if concept in concepts_observed:\n",
    "                # Already seen this concept for this patient\n",
    "                continue            \n",
    "                \n",
    "            domain = df_concepts.loc[concept, 'domain_id']\n",
    "            cohort_domain_concept_counter[domain][concept] += 1\n",
    "            concepts_observed.append(concept)\n",
    "            \n",
    "    learned_disease_profiles[disease_subtype] = cohort_domain_concept_counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the stats of each concept that is observed in > 20% of each disease subtype\n",
    "print('Note: subtype percentages can add up to > 100% since different timelines can be extracted from each patient to represent a disease subtype')\n",
    "for disease_subtype, cohort_domain_concept_counter in learned_disease_profiles.items():        \n",
    "    n_disease_subtype = len(patients_learned_disease_subtype[disease_subtype])\n",
    "    print(f'{disease_subtype}: {n_disease_subtype} / {n_patients} = {(n_disease_subtype / n_patients * 100):.02f}')\n",
    "    \n",
    "    for domain, concept_counter in cohort_domain_concept_counter.items():\n",
    "        print(domain)\n",
    "        \n",
    "        # Create a DataFrame from the counts\n",
    "        df_subtype_domain_counts = pd.DataFrame.from_dict(concept_counter, orient='index', columns=['count'])\n",
    "        df_subtype_domain_counts.index.name = 'concept_id'\n",
    "        \n",
    "        # Calculate prevalence of each concept within the disease subtype\n",
    "        df_subtype_domain_counts['prevalence'] = df_subtype_domain_counts['count'] / n_disease_subtype * 100\n",
    "        \n",
    "        # Add concept_name to DataFrame\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.join(df_concepts['concept_name'], how='left')\n",
    "        \n",
    "        # Re-arrange column order,  sort by descending order of count, and display\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts[['concept_name', 'count', 'prevalence']]\n",
    "        df_subtype_domain_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "        display(df_subtype_domain_counts.loc[((df_subtype_domain_counts['prevalence'] > 20) & (df_subtype_domain_counts['count'] > 20)), :].head(20))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
